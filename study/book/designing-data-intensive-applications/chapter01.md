# Ch.1 Reliable, Scalable, and Maintainable Applications  

## What is expected to be learned from this chapter?
- Reliability 의 정의와, 이를 발생시킬 수 있는 하드웨어, 소프트웨어, 휴먼 에러 등
- Scalability 의 정의와, Load, performance 의 정의 그리고 이 로드를 처리해 확장성을 키우는 것의 의미
- Maintainability 의 의미와, 이와 연관된 3가지 개념
- figure 1-1 같은 설계를 해야하는 이유와 목적, 그리고 해당 설계의 장단점
- 2012년 당시 트위터의 데이터 파이프라인이 왜 효율적이었는지, 어떤 개선이 필요한지
- 
## Questions to be answered as I read the book
- figure 1-1 의 Full-Text index는 뭘까?
- 하드웨어의 문제를 어떻게 소프트 웨어 개발자가 핸들하려하는 걸까?
  - 하드웨어 중복성을 늘리고 소프트웨어의 결함내성을 올리는 기술을 이중으로 적용하는게 일반적
- 확장성과 load 의 의미가 요청일까? 데이터의 양일까?
  - 둘 다 책에서 말하는 load parameter가 될 수 있으며, 상황에 따라 서비스에 더 중요하다고 판단되는 파라미터를 우선하거나 다 챙기거나 할 수 있을 것 같음
- 퍼포먼스에 대한 설명중 리스폰스 타임에 대한 일러스트레이션이 나온 이유는?
  - 응답 시간은 온라인 시스템에서 사용하는 퍼포먼스의 지표이며, 이가 어떻게 영향 받을 수 있고 이 지표를 어떻게 분석할 것인지를 확인하기 위해.
- Percentile 의 중요성은 무엇이며, 왜 이걸 퍼포먼스 쪽에서 길게 설명을 하며 강조하는가?
  - 백분율을 통해 얻을 수 있는 지표들에 대한 분석이 가능하기 때문으로, 이걸 고려해야하는 이유는 시스템의 
- Functional Requirements VS Nonfunctional Requirements?
  - Functional Requirements 는 직접 다루는 내용은 아님.
  - Nonfunctional Requirements 중 신뢰성, 확장성, 유지보스성을 다름. 자세한 내용은 후술.

----

### Thinking About Data Systems
- 데이터 스토어등 (디비, 메세지 큐 등)은 굉장히 비슷한 면이 있음에도 굉장히 다른 액세스 패턴을 가지고, 액세스 패턴이 다르기에 **성능적 특성이 다르고, 구현방식도 다름**
- 메세지 큐의 역할을 할 수 있는 레디스, 디비-라잌 durability 를 가진 카프카 등이 있다. 각 카테고리간의 차이가 희미해져간다.
- 최근에는 하나의 특성을 가진 툴로는 부족한 경우가 많기에 어플리케이션 코드에서 여러 툴을 사용함 (Figure 1-1)

### Reliability
- 소프트웨어에서 말하는 안정성: _"뭔가 잘못된 일이 생기더라도 계속 정상적으로 돌아감"_
  - 어플리케이션이 유저의 예상대로 동작함
  - 유저의 실수나, 예상되지 않은 사용 (버그, 어뷰징) 방법으로 사용해도 버팀
  - 예측한 로드와 데이터 양에 필요한 유스케이스에 맞는 퍼포펀스를 가짐
  - 어뷰징/비인증 엑세스를 방
- Faults (결함)과 Failure (장애)의 차이:
  - 결함: 하나의 컴포넌트가 정해진 스펙대로 동작하지 않음
  - 장애: 시스템 전체가 유저에게 서비스할 수 없음
- 결함을 완전히 없애는 것은 불가능함. -> 결함에 대한 내성 (tolerance) 를 가지는 것이 좋음
- 보안 문제의 경우는 돌이킬 수 없기 때문에 결함 내성을 가지기 보단 결함을 방지해야한다.

#### Hardware Faults
- 큰 데이터 센터등지에서는 하드웨어 이슈 (네트워크 선 뽑힘 등) 이 매우 많이 발생한다는 듯 하다
- 하드웨어 중복성을 늘리고 소프트웨어의 결함내성을 올리는 기술을 이중으로 적용하는게 일반적

#### Software Faults
- 체계적인 오류 (Systemic faults) 란?
  - 잘못된 특정 입력으로 인한 모든 애플리케이션 서버 인스턴스의 다운
  - CPU time, memory, disk space, network 대역폭 등 공유 자원을 과도하게 사용하는 일부의 프로세스
  - 이에 대한 해결책:
    - 빠른 해결책은 없고, 
#### Human Errors
- 2003년의 조사된 내용이지만, 설정 오류가 중단의 주요원인임. (하드웨어 결함은 10~25% 정도)
  - 이때는 배포 자동화 기술이 부족해서 발생한 이슈는 아니었을까?
- 예방하는 방법:
  - 시스템을 에러의 가능성이 최소화되게 디자인해라
  - 실수가 많은 부분에서 장애가 발생할 수 있는 부분을 분리. (sandbox를 제공하여 안전한 테스트를 진행)
    - sandbox란?
  - 모든 레벨의 테스트를 철저히 하라. 코너 케이스에 대한 테스트도 잊지 말고 진행
  - 빠르고 쉽게 복구할 수 있게 만들어라
  - 모니터링 대책의 마련.

### Scalability
- 부하(Load)가 증가할 때 시스템이 대처할 수 있는 능력

#### Describing Load
- 부하 매개 변수 (Load Parameters): 
  - 시스템의 아키텍처에 따라 사용할 수 있는 수치/숫자
  - ex: 초당 웹서버 요청 수, DB 읽기/쓰기 비율, 동시 접속자 수 등 
- 트위터 예시:
  - 트윗 작성 -> 팔로워가 그 트윗을 봐야함 (홈타임라인)
  - 두가지 방법 중, 트위터가 2번째 방법 (트윗 작성시 모든 팔로워의 홈 타임라인 캐시에 새로운 트윗을 삽입) 을 선택한 이유는 뭘까? 
    - 트윗 작성의 초당 요청은 피크에 1만2천 vs 홈 타임라인은 초당 30십만 건의 요청이 있었기 때문
      - 하지만 이 방식의 문제점도 있다 (엣지 케이스가 되는 유저들의 팔로워는 3천만이 넘고, 이 경우 하나의 트윗에 대해 3천만건의 쓰기 작업이 발새해 버린다. 이는 5초 이내에 팔로워에게 트윗을 전송해야하는 트위터 입장에선 새로운 문제) 
        - 이를 해결하기 위해 혼합형 방식으로 변경 중 (일부 엣지 케이스가 되는 유저들의 경우에는 팬 아웃에서 제외시키고 팔로워가 홈 타임라인을 호출할때 데이터베이스에서 직접 SELECT)
    - 이러한 판당의 기준이 될 수 있게 해주는게 load parameter이고, 이 예시에서는 _초당 요청량이 load parameter_ 

#### Describing Performance
- 지표:
  - 처리량 (Throughput)
    - 초당 처리되는 레코드 수, 데이터 집합에 대한 작업의 소요시간 
    - 배치성 시스템 (hadoop) 에서는 처리량을 지표로 사용함
  - 응답 시간 (response time)
    - 클라이언트가 요청을 보내고 받는 사이의 시간
    - 온라인 시스템에서는 응답 시간이 더 중요한 지표
- 지연(latency)과 응답 시간(response time)의 차이:
  - 지연: 요청이 처리되기를 기다리며 휴지 상태가 된 시간
  - 응답 시간: 클라이언트 입장에서 응답을 받을 때까지 실제 소요된 시간. 네트워크 지연, 큐 지연을 모두 포함.
- 측정 방법:
  - mean (평균) 보다는 percentile(백분위)을 권장
    - median (중앙값, p50): **대부분의 유저의 대기 시간으로 삼기 좋은 지표** 
      - 절반의 유저들은 이 시간보다 적게 기다리고, 절반의 유저는 이것보다 많이 기다린다.
    - tail latencies (95p, 99p, 99.9p): 시스템의 최악의 경우를 상정하게 해주는 지표
      - 95/99/99.9%의 유저들은 이 대기 시간 보다는 빠르게 응답을 받을 수 있다는 지표로 삼을 수 있음
    - 서비스 수준 목표(Service Level Objective, SLO) 나 서비스 수준 협약서(Service Level Agreement, SLA) 에서 사용함.
      - SLA ex: 대부분의 요청에 대한 응답 시간은 n밀리초 이하여야하고 99분위가 1초 미만일 때 시스템이 정상이라고 규정한다.
      
#### Approaches for Coping with Load
- 하나의 부하 레벨에 적절한 아키텍처는 보통 10배의 부하를 감당할 수는 없다.
- 언젠가 미래에는 큰 볼륨의 데이터나 트래픽을 다루지 않아도 분산 데이터 시스템이 기본이 될지도 모른다..?
- 대부부의 대규모로 동작하는 시스템 아키텍쳐는 어플리케이션 특화이고, 만능은 없다
  - ex: 개당 1kB + 초당 10만건 vs 개당 2 GB + 분당 3건
    - 위 두 케이스 모두 분당 약 6기가의 처리량을 가지지만, 두 시스템을 위한 설계는 완전히 다를 것

### Maintainability
- 우리의 목표: 유지 보수의 고통을 줄이고, 레거시 소프트웨어를 최소화
- 3대 설계 원칙:
  - Operability: 운영팀에서 시스템을 원활히 운영할 수 있도록 해야한다.
  - Simplicity: 새로운 엔지니어들이 시스템을 이해하기 쉽게 복잡도를 최대한 없애야한다
  - Evolvability: 미래에 있을 변화에 열려있어야 한다.

#### Operability
- 좋은 운영팀들이 보통 책임지는 사항들: 
  - monitoring을 통해 필요시 서비스 복원, 장애/성능 저하 원인 추적 등등..
  - 이를 가능하게 하기 위한 좋은 데이터 시스템이 갖춰야 하는 것:
    - 모니터링을 통한 런타임 동작에 대한 가시성, 자동와 보편적 툴들과의 통합 및 자동화 지원, 특정 단말에의 종속성이 없음 등

#### Simplicity
- 시스템의 복잡도가 발생헀다는 증상들:
  - 상태 공간의 폭발, 모듈간 강결합, 복잡한 의존성, 일관성 없는 네이밍 등
- 추상화를 통해 단순화를 시킬 수 있음
  - 세부 구현을 숨기고, 어떤 결과를 예측할 수 있을지만 바로 알 수 있게 제공

#### Evolvability
- 미래에 요구사항의 변화에 시스템을 쉽게 적응시키는 능력
- 주로 Agile 패턴, TDD등의 리팩터링을 활용해 해결됨